{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OAuth Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to scrape twitter data and do a tf-idf analysis on that (src-uwes twitter analysis). We will need OAuth authentication, and we will follow a similar approach as detailed in the yelp analysis notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import oauth2 as oauth\n",
    "import urllib2 as urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oauth.OAUTH_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need twitter api access. The following steps as available online will help you set up your twitter account and access the live 1% stream.\n",
    "\n",
    "1. Create a twitter account if you do not already have one.\n",
    "2. Go to https://dev.twitter.com/apps and log in with your twitter credentials.\n",
    "3. Click \"Create New App\"\n",
    "4. Fill out the form and agree to the terms. Put in a dummy website if you don't have one you want to use.\n",
    "5. On the next page, click the \"API Keys\" tab along the top, then scroll all the way down until you see the section \"Your Access Token\"\n",
    "6. Click the button \"Create My Access Token\". You can Read more about Oauth authorization online. \n",
    "\n",
    "Save the details of api_key, api_secret, access_token_key, access_token_secret in your vaule directory and load it in the notebook as shown in yelpSample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to define the following variables\n",
    "#api_key = #<get api key> \n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "\n",
    "#defining them right here is not safe. insteadm create a file in a different directory\n",
    "# (I use ~/VaultDSE) and in it put a file called, say, twitterkeys.py whose\n",
    "# content is:\n",
    "#api_key = #<get api key>\n",
    "#api_secret = #<get api secret>\n",
    "#access_token_key = #<get your access token key here>\"\n",
    "#access_token_secret = #<get your access token secret here>\n",
    "#\n",
    "#def getkeys():\n",
    "#    return api_key,api_secret,access_token_key,access_token_secret\n",
    "\n",
    "\n",
    "# then use the following commands\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/Deepthi/Documents/DSE/dmysoren/Python Day 5/VaultDSE')\n",
    "import twitterKeys\n",
    "api_key,api_secret,access_token_key,access_token_secret=twitterKeys.getkeys()\n",
    "\n",
    "_debug = 0\n",
    "\n",
    "oauth_token    = oauth.Token(key=access_token_key, secret=access_token_secret)\n",
    "oauth_consumer = oauth.Consumer(key=api_key, secret=api_secret)\n",
    "\n",
    "signature_method_hmac_sha1 = oauth.SignatureMethod_HMAC_SHA1()\n",
    "\n",
    "http_method = \"GET\"\n",
    "\n",
    "http_handler  = urllib.HTTPHandler(debuglevel=_debug)\n",
    "https_handler = urllib.HTTPSHandler(debuglevel=_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a twitter request method which will use the above user logins to sign, and open a twitter stream request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTwitterStream(url, method, parameters):\n",
    "    req = oauth.Request.from_consumer_and_token(oauth_consumer,\n",
    "                                             token=oauth_token,\n",
    "                                             http_method=http_method,\n",
    "                                             http_url=url, \n",
    "                                             parameters=parameters)\n",
    "\n",
    "    req.sign_request(signature_method_hmac_sha1, oauth_consumer, oauth_token)\n",
    "    headers = req.to_header()\n",
    "\n",
    "    if http_method == \"POST\":\n",
    "            encoded_post_data = req.to_postdata()\n",
    "    else:\n",
    "        encoded_post_data = None\n",
    "        url = req.to_url()\n",
    "\n",
    "    opener = urllib.OpenerDirector()\n",
    "    opener.add_handler(http_handler)\n",
    "    opener.add_handler(https_handler)\n",
    "\n",
    "    response = opener.open(url, encoded_post_data)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to request a response as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we will test the above function for a sample data provided by twitter stream here -  \n",
    "url = \"https://stream.twitter.com/1/statuses/sample.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "response = getTwitterStream(url, \"GET\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which will take a url and return the top 10 lines returned by the twitter stream\n",
    "\n",
    "** Note ** The response returned needs to be intelligently parsed to get the text data which correspond to actual tweets. This part can be done in a number of ways and you are encouraged to try different approaches to parse the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 awesome https://t.co/mEXtLGgmPw \n",
      "\n",
      "2 2 2 2 2 2 ÂèóË©±Âô®Ë∂ä„Åó„Å´„Å™„Çã„Å®ÁÖß„Çå„Çã„Åë„Å© „Åù„Çä„ÇÉ‰∏ÄÁ∑í„Åå„ÅÑ„ÅÑ ÂΩì„Åü„ÇäÂâç„Åï ÊÄù„ÅÜ„Åª„Å©‰∏äÊâã„Åè„ÅÑ„Åã„Å™„ÅÑ„Åë„Å© ÈÅ†„ÅèÈÅ†„Åè Èõ¢„ÇåÈõ¢„Çå ÈõªË©±Âàá„Çå„Å™„ÅÑÂ§ú„ÇÇ„ÅÇ„Çã ÈóáÊ∑±„ÅèÂøÉÁ¥∞„Åè „Åù„Çå„Åß„ÇÇÂÖâÂ∞Ñ„Åó Ê≥£„ÅÑ„Åü„ÇäÁ¨ë„Å£„Åü„Çä„Åï Âà•„ÅÆË°ó„Å´ÊöÆ„Çâ„ÅôÂêõ„Çà ÂØÑ„ÇäÊ∑ª„Å£„Å¶„ÇÑ„Çå„Å™„ÅÑ„Åå ÂÉï„ÅØ„Åì„Åì„Å´„ÅÑ„Çã&lt;‚ô™ÈõªË©±/„É¨„Éü„Ç™„É≠„É°„É≥&gt; \n",
      "\n",
      "3 RT @AlDubBarkads: Eto pa! Pak! Fresh look! (¬©Mr Andy Sylvia's Fb) OMG! Diko kinakaya! Bagay na bagay! ‚ù§ #ALDUBPromiseRing https://t.co/QAHY‚Ä¶ \n",
      "\n",
      "4 RT @TheKateWolff: Please don't make a duck face in your breast feeding picture. \n",
      "\n",
      "5 RT @KatyTurNBC: \"Most of the media is absolute scum\" - Trump \n",
      "\n",
      "6 ÊúùÂÖÖÈõª100%„ÅßÊúù„Åã„Çâ„Åù„Çì„Å™Êê∫Â∏Ø‰Ωø„Å£„Å¶„Å™„ÅÑ„ÅÆ„Å´„ÇÇ„ÅÜ45%„ÇÑ„Å≠„Çì„Åë„Å© \n",
      "\n",
      "7 RT @OneDrecti0nFans: \"We're like....5 best friends\" \n",
      "\n",
      "I AM GONNA CRY :')\n",
      "\n",
      "#MTVStars One Direction \n",
      "\n",
      "https://t.co/lZkGrI25QQ \n",
      "\n",
      "8 „Éö„Éº„ÇπÈÖçÂàÜ„Å®„ÅÑ„ÅÜË®ÄËëâ„ÇíÁü•„Çâ„Å™„ÅÑ„ÅÆ„ÅßÊîπ‰øÆÂ∑•Âª†Âá∫„Åó„Å¶Âç≥12.7cmÈ´òËßíÁ†≤+È´òÂ∞ÑË£ÖÁΩÆ‰Ωú„Å£„Å¶„Éç„Ç∏‰Ωø„ÅÑÊûú„Åü„Åó„Åü \n",
      "\n",
      "9 Se dizem amigos mais n√£o me respondem, chega glr \n",
      "\n",
      "10 RT @Natynevees: @AnabelBezerra @any_sacana era de vcs sim \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get top 10 tweets returned by the twitter stream\n",
    "\n",
    "## This is a simple fetchData() function which can be used to get the top 10 tweets without any criteria. \n",
    "## This is implemented using the url given above\n",
    "\n",
    "import json\n",
    "\n",
    "def fetchData(url):\n",
    "    i=0\n",
    "    parameters = []\n",
    "    response = getTwitterStream(url, \"GET\", parameters)\n",
    "    while(i<10):\n",
    "        try:\n",
    "            data = json.loads(response.readline())\n",
    "            print i+1, data['text'], '\\n'\n",
    "            i+=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "fetchData(\"https://stream.twitter.com/1/statuses/sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also request twitter stream data for specific search parameters as follows\n",
    "# url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fetchData function to fetch latest live stream data for following search queries and output the first 5 lines\n",
    "\n",
    "1. \"UCSD\"\n",
    "2. \"Donald Trump\"\n",
    "3. \"Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******URL generated to obtain results for UCSD https://api.twitter.com/1.1/search/tweets.json?q=UCSD&count=5 \n",
      "\n",
      "FOLLOWING ARE THE TOP 5 SEARCH QUERIES:\n",
      "1 UCSD student leaders criticized for cutting media funds: UCSD student leaders criticized for cutting funds to the‚Ä¶ https://t.co/IlQYWLSPPP \n",
      "\n",
      "2 RT @arankhanna: Interesting (albeit quite technical) paper from @UCSDJacobs on how to de-anonymize users with webGL fingerprinting https://‚Ä¶ \n",
      "\n",
      "3 RT @arankhanna: Interesting (albeit quite technical) paper from @UCSDJacobs on how to de-anonymize users with webGL fingerprinting https://‚Ä¶ \n",
      "\n",
      "4 UCSD student leaders are facing some backlash for cutting funding to the Koala and other campus publications. https://t.co/Z3pkiGtWDj \n",
      "\n",
      "5 RT @UCSDtritons: #UCSD #AOTW are @ucsd_wvb Amanda Colla and @UCSDTritonsMWP Chase Cockerill.üî± https://t.co/jMBfZS7kYt @USECreditUnion https‚Ä¶ \n",
      "\n",
      "*******URL generated to obtain results for Syria https://api.twitter.com/1.1/search/tweets.json?q=Syria&count=5 \n",
      "\n",
      "FOLLOWING ARE THE TOP 5 SEARCH QUERIES:\n",
      "1 #ISIS terrorists in #Iraq and #Syria 'may be using weapons exported by the #UK' https://t.co/p12h3CT1EH @independent #Cameron #DestroyISIS \n",
      "\n",
      "2 RT @Marthinuscademy: #Russia #Putin asks #UNSC to hold closed-door meeting on #Turkey actions in #Iraq and #Syria https://t.co/a32HWfpY0U h‚Ä¶ \n",
      "\n",
      "3 RT @Free_Media_Hub: And finally Syria the afternoon will bring a heavy shower of bombs followed by a firestorm of death https://t.co/wOcu67‚Ä¶ \n",
      "\n",
      "4 Reporting from Syria: Wael‚Äôs Story https://t.co/C44zUfR1BF \n",
      "\n",
      "5 RT @AmirAmir2015: Aid reaches rebel-held area of Homs, Syria https://t.co/wjhDt0GHJ2 #LeMonde #iran #Syria \n",
      "\n",
      "*******URL generated to obtain results for Donald Trump https://api.twitter.com/1.1/search/tweets.json?q=Donald Trump&count=5 \n",
      "\n",
      "FOLLOWING ARE THE TOP 5 SEARCH QUERIES:\n",
      "1 RT @leobolona: Who would make a better president of the United States?\n",
      "RT for Cardi B. Fav for Donald Trump. https://t.co/RfQxv2CeGC \n",
      "\n",
      "2 RT @businessinsider: 'We have no choice': Donald Trump touts his plan to stop Muslims from entering the US https://t.co/V2DRXYMJ1g \n",
      "\n",
      "3 RT @teganandsara: Donald Trump is absolute garbage. Racist, hateful and scary. How could anyone support this man. Shame. \n",
      "\n",
      "4 RT @whytruy: look at this, this is why we have to make sure donald trump does not win the election. https://t.co/RSFe9tXVAA \n",
      "\n",
      "5 RT @ABC: Donald Trump defends barring Muslims from US: \"We have no idea if they love us or hate us.\" https://t.co/8GP8o6K3Ee https://t.co/1‚Ä¶ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## This is a slightly modified fetchdata function which will fetch the data and extract the tweet from a url with\n",
    "## a search query. (the url given above is used)\n",
    "\n",
    "import json\n",
    "\n",
    "def fetchData(url):\n",
    "    parameters = []\n",
    "    response = getTwitterStream(url, \"GET\", parameters)\n",
    "    data = json.loads(response.readline())\n",
    "    print 'FOLLOWING ARE THE TOP 5 SEARCH QUERIES:'\n",
    "    for i in xrange(int(cnt)):\n",
    "        print i+1, data['statuses'][i]['text'],'\\n'\n",
    "        \n",
    "\n",
    "search_query = ['UCSD','Syria','Donald Trump']\n",
    "cnt = '5'\n",
    "for search_query in search_query:\n",
    "    url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query+'&'+'count='+cnt\n",
    "    print '*******URL generated to obtain results for', search_query, url, '\\n'\n",
    "    fetchData(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF###\n",
    "\n",
    "tf‚Äìidf, short for term frequency‚Äìinverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is among the most regularly used statistical tool for word cloud analysis. You can read more about it online (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "We base our analysis on the following\n",
    "\n",
    "1. The weight of a term that occurs in a document is simply proportional to the term frequency\n",
    "2. The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs\n",
    "\n",
    "For this question we will perform tf-idf analysis o the stream data we retrieve for a given search parameter. Perform the steps below\n",
    "\n",
    "1. use the twitterreq function to search for the query \"syria\" and save the top 200 lines in the file twitterStream.txt\n",
    "2. load the saved file and output the count of occurrences for each term. This will be your term frequency\n",
    "3. Calculate the inverse document frequency for each of the term in the output above.\n",
    "4. Multiply the term frequency for each of the term by corresponding inverse document frequency.\n",
    "5. Sort the terms in the descending order based on their term freq/inverse document freq scores \n",
    "6. Print the top 10 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## fetchData() function below fetches the data from the url which is constructed using search query and the counts\n",
    "## In the example below, on 100 tweets are extracted as per clarification on piazza\n",
    "\n",
    "import json\n",
    "\n",
    "def fetchData(url):\n",
    "    parameters = []\n",
    "    response = getTwitterStream(url, \"GET\", parameters)\n",
    "    data = json.loads(response.readline())\n",
    "    newfile = open(\"twitterStream.txt\",\"w\")\n",
    "    for i in xrange(int(cnt)):\n",
    "        tmp = data['statuses'][i]['text']\n",
    "        tmp2 = tmp.encode('utf-8')\n",
    "        newfile.write(tmp2.replace('\\n',' ') +'\\n')\n",
    "\n",
    "search_query = 'syria'\n",
    "cnt = '100' ## Getting 100 tweets as per the clarification on Piazza\n",
    "url= \"https://api.twitter.com/1.1/search/tweets.json?q=\"+search_query+'&'+'count='+cnt\n",
    "twitterStream = fetchData(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>terrorism/genocide</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thats</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stabbing</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ŸàŸÑÿß</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again</th>\n",
       "      <td>6.993015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#buybooks</th>\n",
       "      <td>6.993015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrorism</th>\n",
       "      <td>5.991465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>5.244761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tf_idf\n",
       "word                        \n",
       "terrorism/genocide  7.824046\n",
       "thats               7.824046\n",
       "stabbing            7.824046\n",
       "disgust             7.824046\n",
       "ŸàŸÑÿß                 7.824046\n",
       "money               7.824046\n",
       "again               6.993015\n",
       "#buybooks           6.993015\n",
       "terrorism           5.991465\n",
       "people              5.244761"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tf-idf calculation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get the data from the extracted file\n",
    "text = ! cat twitterStream.txt\n",
    "\n",
    "# Calculate term frequency - tf\n",
    "word_frequency = []\n",
    "for i in range(len(text)):\n",
    "    text[i]=str(text[i])\n",
    "    for char in '-.,[]:\\'{}|?(\")_;<>=!*`':\n",
    "        text[i]=text[i].replace(char,'')    \n",
    "    for char in '\\\\':\n",
    "        text[i]=text[i].replace(char,'')\n",
    "    for char in '\\n':\n",
    "        text[i]=text[i].replace(char,'')\n",
    "    text[i] = text[i].lower()\n",
    "    words = text[i].split()\n",
    "    count1 = {i:words.count(i) for i in set(words)}\n",
    "    new = pd.DataFrame(count1.items(),columns =(['word','frequency']))\n",
    "    word_frequency.append(new)\n",
    "    \n",
    "word_frequency2 = pd.concat(word_frequency, axis=0)\n",
    "\n",
    "\n",
    "words = set(word_frequency2['word'])\n",
    "\n",
    "# Calculate document frequency \n",
    "cnt = 0\n",
    "ne = 0 \n",
    "w_cnt = []\n",
    "for w in words:\n",
    "    with open(\"twitterStream.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            for char in '-.,[]:\\'{}|?(\")_;<>=!*`':\n",
    "                line=line.replace(char,'')\n",
    "            for char in '\\\\':\n",
    "                line=line.replace(char,'')\n",
    "            for char in '\\n':\n",
    "                line=line.replace(char,'')\n",
    "            if w in line:\n",
    "                cnt=1\n",
    "                w_cnt.append((w,cnt,line))\n",
    "            else:\n",
    "                ne+=1\n",
    "        \n",
    "document_freq = pd.DataFrame(w_cnt, columns=(['word','cnt','line']))\n",
    "\n",
    "document_freq2 = document_freq.groupby('word').agg('sum')\n",
    "\n",
    "document_freq2.sort(['cnt'],ascending=0)\n",
    "\n",
    "\n",
    "## Calculate inverse document frequency\n",
    "import math\n",
    "\n",
    "document_freq2['inv_doc']= document_freq2['cnt'].apply(lambda x: math.log(100/(1+x)))\n",
    "\n",
    "result = pd.merge(word_frequency2, document_freq2.reset_index(), how='inner',on=['word','word'])\n",
    "\n",
    "\n",
    "result['tf_idf'] = result['frequency']*result['inv_doc']\n",
    "\n",
    "final_result = result[['word','tf_idf']]\n",
    "\n",
    "## Get the top 10 terms used\n",
    "final_result.groupby('word').agg({'tf_idf':'mean'}).sort('tf_idf',ascending = False).head(10)\n",
    "\n",
    "\n",
    "## If you look at the results it looks like most of the words are those that add value to the tweet except \n",
    "## the word 'thats'. I think that since we are looking into tweets, very few people use the full form of these common \n",
    "## words and hence this is one off case. \n",
    "## Also, if you observe, the word 'Syria' doesn't appear in the top list. This implies that it doesn't add more value \n",
    "## into the document as it is present across all the documents. Thus proving the purpose of tf-idf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
